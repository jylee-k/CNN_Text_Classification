{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "---\n",
    "In this notebook, I'll train a **CNN** to classify the sentiment of movie reviews in a corpus of text. The approach will be as follows:\n",
    "* Pre-process movie reviews and their corresponding sentiment labels (positive = 1, negative = 0).\n",
    "* Load in a **pre-trained** Word2Vec model, and use it to tokenize the reviews.\n",
    "* Create training/validation/test sets of data.\n",
    "* Define a `SentimentCNN` model that has a pre-trained embedding layer, convolutional layers, and a final, fully-connected, classification layer.\n",
    "* Train and evaluate the model.\n",
    "\n",
    "An example of a positive and negative review are shown below.\n",
    "\n",
    "<img src='notebook_ims/reviews_ex.png' width=30% height=70% />\n",
    "\n",
    "The task of text classification has typically been done with an RNN, which accepts a sequence of words as input and has a hidden state that is dependent on that sequence and acts as a kind of memory. You can see an example that classifies this same review dataset using an RNN in [this Github repository](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb). \n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "This example shows how you can utilize convolutional layers to find patterns in sequences of word embeddings and create an effective text classifier using a CNN-based approach.\n",
    "\n",
    "**1. Original paper**\n",
    "* The code follows the structure outlined in the paper, [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) by Yoon Kim (2014). \n",
    "\n",
    "**2. Pre-trained Word2Vec model**\n",
    "\n",
    "* The key to this approach is convolving over word embeddings, for which I will use a pre-trained [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) model. \n",
    "* I am specifically using a \"slim\"-version of a model that was trained on part of a Google News dataset (about 100 billion words). The [original model](https://code.google.com/archive/p/word2vec/) contains 300-dimensional vectors for 3 million words and phrases.\n",
    "* The \"slim\" model is cut to 300k English words, as described in [this Github repository](https://github.com/eyaler/word2vec-slim).\n",
    "\n",
    "You should be able to modify this code slightly to make it compatible with a Word2Vec model of your choosing.\n",
    "\n",
    "**3. Movie reviews data **\n",
    "\n",
    "The dataset holds 25000 movie reviews, which were obtained from the movie review site, IMDb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load in and Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.keys>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../preprocess/arc_split.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../preprocess/nu_split.json\") as f:\n",
    "    nu_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([{'text': 'create a storyboard for a sequel to your book use the same characters', 'label': 2}, {'text': 'briefly describe the authors approach to the liabilities of the goto statement', 'label': 0}, {'text': 'specify a deterministic finite automaton dfa recognizing the', 'label': 1}, {'text': 'how to construct the optimal maintenance problem', 'label': 1}, {'text': 'specify the thickness of a galvanized coating based on atmospheric data level ', 'label': 1}, {'text': 'discuss the nature of socialism', 'label': 0}, {'text': 'what is the title of the first movie you saw ', 'label': 0}, {'text': 'how many years does this animal live', 'label': 0}, {'text': 'when did the first saudi radio station open', 'label': 0}, {'text': 'contrast olympic athletes of today with athletes of past olympic games', 'label': 2}, {'text': 'express the following percentages as fractions in simplest form', 'label': 0}, {'text': 'compare how different children come to school', 'label': 2}, {'text': 'describe the output of this program fragment', 'label': 0}, {'text': 'relate topographic map to field features', 'label': 1}, {'text': 'analyze safe and dangerous aspects of these features', 'label': 2}, {'text': 'design and make an animal that moves', 'label': 2}, {'text': 'compare historical events to contemporary situations', 'label': 0}, {'text': 'classify the following substances as particle atom element molecule compound', 'label': 0}, {'text': 'how do you combine the results of various individual tests to determine whether a student can progress to a next semester phase year', 'label': 2}, {'text': 'how does autonomy differ with age', 'label': 2}, {'text': 'appraise the speechs effectiveness based upon the class criteria', 'label': 2}, {'text': 'predict what happens to x if y increases', 'label': 1}, {'text': 'write a song about old macdonald who had a bulldozer instead of a farm', 'label': 2}, {'text': 'state the characteristics of each type of corrosion testing studied', 'label': 0}, {'text': 'following the identification of the forms of corrosion that were involved in a failure recommend a solution to avoid this failure', 'label': 2}, {'text': 'organize this book into three or more sections and give your own subtitle for each section', 'label': 2}, {'text': 'what can i do to investigate this problem further', 'label': 2}, {'text': 'paraphrase an important speech', 'label': 0}, {'text': 'describe what happened at', 'label': 0}, {'text': 'pretend you are a librarian recommending this book to someone write a paragraph telling what you would say', 'label': 2}, {'text': 'using information from the book about one of the main characters rewrite the ending of the book', 'label': 2}, {'text': 'correctly label the brain lobes indicated on the diagram below', 'label': 0}, {'text': 'describe an experiment to answer the question of the effects of weight on the fall of an object', 'label': 1}, {'text': 'make up a new language code and write material using it', 'label': 2}, {'text': 'what are the representational symbols on maps and charts', 'label': 0}, {'text': 'name one character rewrite the story from this characters point of view', 'label': 2}, {'text': 'revises and process to improve the outcome', 'label': 2}, {'text': 'evaluate your own or a peers essay in terms of the principles of composition discussed during the semester', 'label': 2}, {'text': 'how does compare contrast with', 'label': 2}, {'text': 'describe in your own words how to borrow a book from the library', 'label': 0}, {'text': 'can you distinguish aged rums from unaged rums by smell', 'label': 2}, {'text': 'was hemingway a great american writer first you will need to define greatness', 'label': 2}, {'text': 'how do the clauses and expressions given in the article which replace goto statements lead to the three traditional control statements', 'label': 2}, {'text': 'how to combine virtual symmetry and symbolic model checking effectively', 'label': 2}, {'text': 'compose a complete c program that reads text strings from a text file into a suitable data', 'label': 2}, {'text': 'can you find four different feelings pa lia had during the story', 'label': 2}, {'text': 'construct a poster that will advertise your new food product in an exciting and irresistible way', 'label': 2}, {'text': 'use a sketch too find the exact value of each expression', 'label': 1}, {'text': 'given an argument on any position enumerate the logical fallacies in that argument', 'label': 2}, {'text': 'choose a country that does not compete at the olympics and explain why that country is not an olympic member', 'label': 1}, {'text': 'how could we determine the number of pennies in a jar without counting them', 'label': 2}, {'text': 'show through roleplay the final scene in the novel', 'label': 1}, {'text': 'solve for the ten following fraction multiplication problems please make sure to show all your work', 'label': 1}, {'text': 'evaluate two internet sources of information about the egyptians which would be a better choice for your purpose and why', 'label': 2}, {'text': 'which of the two algorithms bubblesort or quicksort is more efficient justify your answer', 'label': 2}, {'text': 'demonstrate how your units specific student learning outcomes are linked to the mission of uam', 'label': 1}, {'text': 'distinguish between the following giving suitable examples', 'label': 2}, {'text': 'what is a textual index', 'label': 0}, {'text': 'how would you decide about', 'label': 2}, {'text': 'compute the area of actual circles', 'label': 1}, {'text': 'compare two of the characters in this book', 'label': 2}, {'text': 'design the architecture of the software system based on the requirements defined in the software requirement specification document', 'label': 2}, {'text': 'in what ways is oxyperoxidase similar to other oxygenated heme proteins and how does it differ', 'label': 2}, {'text': 'what might happen if you combined', 'label': 2}, {'text': 'after solving a problem determine the degree to which the problem was solved as efficiently as possible ', 'label': 2}, {'text': 'justify the title to kill a mockingbird ', 'label': 2}, {'text': 'why dont you devise your own way to deal with ', 'label': 2}, {'text': 'how would you relate position concept with position strategy', 'label': 1}, {'text': 'investigate innovations that can enhance future olympics', 'label': 2}, {'text': 'structure sorts the list in ascending order displays the list on the screen and stores the list', 'label': 2}, {'text': 'how much is your weight', 'label': 0}, {'text': 'rewrite the sentence in your head following the direction given below', 'label': 0}, {'text': 'design a healthy menu that you think most people would enjoy using the healthy eating guide', 'label': 2}, {'text': 'show how ecrm can be used to improve marketing positioning as explained in the article', 'label': 1}, {'text': 'if interest were compounded monthly instead of daily what would the difference in interest be', 'label': 2}, {'text': 'in which countries is arabic language spoken', 'label': 0}, {'text': 'who was the writer of the hamilton ', 'label': 0}, {'text': 'relate the principle of reinforcement to classroom interactions', 'label': 1}, {'text': 'how to construct the selected information system', 'label': 1}, {'text': 'identify one problem in the book and give an alternate solution one not given by the author', 'label': 2}, {'text': 'create a new product give it a name and plan a marketing campaign', 'label': 2}, {'text': 'from a blueprint to describe the article depicted', 'label': 0}, {'text': 'select the most effective solution', 'label': 2}, {'text': 'examine carefully the given map extract on the following page to help you work out the exercise below', 'label': 2}, {'text': 'specify the thickness of a galvanized coating based on atmospheric data level ', 'label': 1}, {'text': 'predict whether phillip will ever go back to visit the cay after several years of recovery assuming he gets his sight back', 'label': 2}, {'text': 'show the value of x after running this program fragment', 'label': 1}, {'text': 'label any olympic sporting apparatus with design features', 'label': 0}, {'text': 'describe in prose what is shown in graph form', 'label': 0}, {'text': 'how do i differentiate between deceive and cheat on', 'label': 2}, {'text': 'what is the setting of to kill a mockingbird', 'label': 0}, {'text': 'state in your own words where in the library to find previous issues of journals that are no longer in the display racks', 'label': 0}, {'text': 'what are the major products and exports of countries', 'label': 0}, {'text': 'define stream bank floodplain and substrate', 'label': 0}, {'text': 'select the athlete of the century and analyze why you chose this person', 'label': 2}, {'text': 'compare and contrast the waterfall model with the prototyping model', 'label': 2}, {'text': 'how does your agency inspect against its expectation for operators ', 'label': 2}, {'text': 'how much water does an elephant drink', 'label': 0}, {'text': 'demonstrate how this could work in an industry setting', 'label': 1}, {'text': 'use the unification algorithm to determine whether the following two terms are uniable show intermediate step', 'label': 1}, {'text': 'summarize this magazine article', 'label': 0}, {'text': 'explain how the biological concept of symbiotic relationships could be used to help solve socially created problems like water pollution overflowing garbage landfills or homelessness', 'label': 2}, {'text': 'contrast the valles marineris on mars with the grand canyon', 'label': 2}, {'text': 'state how to find a word in a dictionary', 'label': 0}, {'text': 'compose a simple rap or rhyme about zoo animals', 'label': 2}, {'text': 'investigate whether or not the system can be solved for x y z ', 'label': 2}, {'text': 'how effectively did students analyze course content on race class and privilege through autobiographical reflection', 'label': 2}, {'text': 'develop a plan for a new olympic bid system', 'label': 2}, {'text': 'can you distinguish pot still rums from column still rums by taste', 'label': 2}, {'text': 'explain in your own words what do you mean by the term economics', 'label': 0}, {'text': 'how are the adolescents with asd similar of different from each other in their sources of communication breakdowns ', 'label': 2}, {'text': 'explain in your own words what is meant by mercantilism', 'label': 0}, {'text': 'place the following in order of priority', 'label': 2}, {'text': 'which part of a data packet is used by a router to determine the destination network', 'label': 0}, {'text': 'what happens if you change the direction of the displacement but not its size', 'label': 1}, {'text': 'label the parts of the microscope shown on the right', 'label': 0}, {'text': 'what approach would you use to prove the truth of the below statement ', 'label': 1}, {'text': 'list three characteristics that are unique to the cubist movement', 'label': 0}, {'text': 'categorise the pictures and add them to the wall display', 'label': 1}, {'text': 'predict the differences of a planet with no seasonal changes', 'label': 2}, {'text': 'describe what goes in each of the four areas on the first page of notes', 'label': 0}, {'text': 'given three possible approaches to implement the defined system discuss the possible advantages and disadvantages of each approach', 'label': 2}, {'text': 'explain the communicative property', 'label': 0}, {'text': 'what does ict stands for', 'label': 0}, {'text': 'apply and integrate several different strategies to solve a mathematical problem not according to one formula', 'label': 2}, {'text': 'determine the next number in a sequence', 'label': 0}, {'text': 'evaluate whether their model is a true representation of the local environment', 'label': 2}, {'text': 'who is the first president of usa', 'label': 0}, {'text': 'listen to two classmates conversing on tape and critique their performance on the basis of the skills covered this semester', 'label': 2}, {'text': 'compare and contrast preview questions clarifying questions and anticipated exam questions including how each relates to the notetaking process ', 'label': 2}, {'text': 'recall basic math facts', 'label': 0}, {'text': 'develop a way to teach the concept of adjectives', 'label': 2}, {'text': 'how is scientific progress similar to darwinian evolution according to kuhn', 'label': 2}, {'text': 'assess the appropriateness of an authors conclusions based on the evidence given', 'label': 2}, {'text': 'explain the causes of historical events', 'label': 0}, {'text': 'design costumes for the characters', 'label': 2}, {'text': 'which technology is used by laser printer', 'label': 0}, {'text': 'revise and process to improve the outcome', 'label': 2}, {'text': 'make a scrapbook about the areas of study', 'label': 1}, {'text': 'draw a painting that uses various principles of perspective to achieve its effect', 'label': 2}, {'text': 'can you compare your with that presented in ', 'label': 2}, {'text': 'break down the components of a standard film camera and explain how they interact to make the machine work', 'label': 2}, {'text': 'write a set of rules to prevent what happened in the story', 'label': 2}, {'text': 'in what ways are foxes similar to wolves and dogs', 'label': 2}, {'text': 'identify a property commutative additive etc', 'label': 0}, {'text': 'paraphrase what hamlet is saying in his soliloquy', 'label': 0}, {'text': 'restate main idea of story in own words', 'label': 0}, {'text': 'how do we differentiate between an interview and an interrogation', 'label': 2}, {'text': 'award the contract to the best proposal rank the principles of good sportsmanship in order of importance to you', 'label': 2}, {'text': 'describe and compare the role of islamic law in the legal systems of malaysia and pakistan', 'label': 0}, {'text': 'evaluate a work of art giving the reasons for your evaluation', 'label': 2}, {'text': 'how could someone modify the cornell method to make it effective for researching a topic for a paper', 'label': 1}, {'text': 'please label the diagram below', 'label': 0}, {'text': 'estimate the difficulty of the question', 'label': 2}, {'text': 'how would the usa be different if the south had won the civil war', 'label': 2}, {'text': 'create plan of local environment by drawing around boxes', 'label': 2}, {'text': 'about what proportion of the population of the us is living on farms', 'label': 0}, {'text': 'which of the following item does not belong to the set ', 'label': 0}, {'text': 'if your story happened in a foreign land compare that land to the united states', 'label': 2}, {'text': 'create a new song for the opening line of mary had a little lamb', 'label': 2}, {'text': 'make up a puzzle game using the ideas from the study area', 'label': 1}, {'text': 'compose an original work which incorporates five common materials in sculpture', 'label': 2}, {'text': 'judge aesthetic qualities and relationship to future values', 'label': 2}, {'text': 'which of the following actions would probably be least effective in determining the answer ', 'label': 2}, {'text': 'express your opinion of drugs in sport through poetry', 'label': 0}, {'text': 'how could we develop a coherent measure of success for organisations engaged in this type of activity with young people', 'label': 2}, {'text': 'estimate expenditure on hivaids education', 'label': 2}, {'text': 'can you solve the following problem ', 'label': 1}, {'text': 'name the property that states that', 'label': 0}, {'text': 'apply the rule of law to a new situation', 'label': 1}, {'text': 'describe in your own words what is meant by a sprained ankle', 'label': 0}, {'text': 'explain the possible uses described in class for the space to the left of the vertical line', 'label': 0}, {'text': 'predict what will happen next in', 'label': 2}, {'text': 'prepare a book jacket that illustrates the kind of book as well as the story', 'label': 2}, {'text': 'would you have liked to have had cinderella for a sister explain why or why not', 'label': 2}, {'text': 'how would you investigate this patient', 'label': 2}, {'text': 'evaluate appropriate and inappropriate actions of characters', 'label': 2}, {'text': 'make a list of facts you learned from the story', 'label': 0}, {'text': 'in your own words how would you summarize the story up ', 'label': 0}, {'text': 'evaluate the corrosion monitoring needs of a chemical processing plant ', 'label': 2}, {'text': 'analyze the characteristics of frogs', 'label': 2}, {'text': 'can you group by characteristics such as ', 'label': 1}, {'text': 'construct a venn diagram to determine the validity of the given argument', 'label': 1}, {'text': 'design a poster for this book', 'label': 2}, {'text': 'what factors would you change if ', 'label': 1}, {'text': 'list the levels in blooms taxonomy', 'label': 0}, {'text': 'how many loop statement available in c', 'label': 0}, {'text': 'classify types of corrosion inhibitors ', 'label': 0}, {'text': 'can you design a to ', 'label': 2}, {'text': 'draw the flow chart of the pnk system', 'label': 0}, {'text': 'defend your use of a specific strategy in solving the problem', 'label': 2}, {'text': 'compare this book to the last book you read', 'label': 2}, {'text': 'how do you construct foliations on a given manifold m', 'label': 1}, {'text': 'can you differentiate between ethnic chinese and ethnic koreans', 'label': 2}, {'text': 'how do you think one should reason in making ethical decisions in business', 'label': 2}, {'text': 'recall the equation for the ideal gas law', 'label': 0}, {'text': 'what is part of this book did you like best tell why you like it', 'label': 2}, {'text': 'what season are we in right now', 'label': 0}, {'text': 'retell the story in your words', 'label': 0}, {'text': 'tell in your own words how the setting of the story made it more interesting', 'label': 0}, {'text': 'conclude and support which economic system leads to a higher standard of living', 'label': 2}, {'text': 'describe nuclear transport to a lay person', 'label': 0}, {'text': 'apply the storytelling technique here to a little story of your own', 'label': 1}, {'text': 'differentiate between call by value and call by reference', 'label': 2}, {'text': 'decide whether you are in favor of building on a floodplain defend your position in a debate', 'label': 2}, {'text': 'create and perform a play about frogs', 'label': 2}, {'text': 'rewrite a part in the poem using a different speaker', 'label': 0}, {'text': 'would you solve these two problems using similar approaches', 'label': 1}, {'text': 'identify the correct definition of osmosis', 'label': 0}, {'text': 'by comparing the map of the tectonic plates to the earthquake map what inferences can you make', 'label': 2}, {'text': 'how can we combine and abstract facts about a software system to create new knowledge', 'label': 2}, {'text': 'describe types of coupling in software design', 'label': 0}, {'text': 'state in your own words the rule for balls and strikes in baseball', 'label': 0}, {'text': 'translate an equation into a computer spreadsheet', 'label': 0}, {'text': 'identify ordinal positions', 'label': 0}, {'text': 'rewrite the markov matrix to account for this slight change', 'label': 0}, {'text': 'invent a machine to do a specific task', 'label': 2}, {'text': 'defend the following claim the cornell method works so well that it could turn even a poor lecture into a valuable learning experience', 'label': 2}, {'text': 'select the best proposal for a proposed water treatment plant', 'label': 2}, {'text': 'given the data weve looked at on this topic evaluate how appropriate this conclusion is and defend your answer', 'label': 2}, {'text': 'estimate the epicentral coordinates of the earthquake', 'label': 2}, {'text': 'decide whether you learned enough about electricity from this book', 'label': 2}, {'text': 'assess the relative effectiveness of different graphical representations of the same data or biological concept', 'label': 2}, {'text': 'construct a market strategy for your product using a known strategy as a model', 'label': 1}, {'text': 'if notes are available on the professors website how should those notes be used', 'label': 1}, {'text': 'analyze the problem identifying its variables', 'label': 2}, {'text': 'suggest ways in which it could have responded more effectively', 'label': 2}, {'text': 'explain the term conjugal families by making reference to the different types of societies to which they could belong', 'label': 2}, {'text': 'devise plans to market or make artwork more valuable', 'label': 2}, {'text': 'construct the data path of the risc processor given in the previous question ', 'label': 1}, {'text': 'translate the following passage from the iliad into english', 'label': 0}, {'text': 'inspect the pattern of gps positions relative to the known published value', 'label': 2}, {'text': 'when was the first airport in the yemen built', 'label': 0}, {'text': 'can you see a possible solution to ', 'label': 2}, {'text': 'use the distance travelled and cost of petrol to calculate the cost of a coach trip using a spreadsheet', 'label': 1}, {'text': 'name the main characters in the story', 'label': 0}, {'text': 'write a short story relating a personal experience in the style of a picaresque novel', 'label': 2}, {'text': 'design a building according to given specifications', 'label': 2}, {'text': 'compare calliope with howie use the word bank', 'label': 0}, {'text': 'using straight value depreciation decide between two coppernickel alloys for the design of a heat exchanger ', 'label': 2}, {'text': 'how would you investigate this abnormality', 'label': 2}, {'text': 'sketch a graph of velocity versus time for the upward and downward parts of a balls flight', 'label': 1}, {'text': 'make a diagnosis or analyze a case study', 'label': 2}, {'text': 'describe the economic consequence of a neolocal society support your description with information you have learned from this course', 'label': 2}, {'text': 'explain the whole method of crushing', 'label': 0}, {'text': 'how might lifestyles in the united states be modified if a more socialistic system were adopted', 'label': 1}, {'text': 'which of the following is the method for determining the volume of a cylinder', 'label': 0}, {'text': 'list three predominant economic systems that exist', 'label': 0}, {'text': 'how would you change the story to create a different ending', 'label': 2}, {'text': 'solve a math problem using familiar procedure or formula', 'label': 1}, {'text': 'judge whether olympic ideals are realistic or unrealistic for the contemporary elite athlete', 'label': 2}, {'text': 'how many states are there in usa', 'label': 0}, {'text': 'create an equation to represent the solution to this problem', 'label': 2}, {'text': 'would you classify the zener diode in this circuit as a series voltage regulator or a shunt voltage regulator', 'label': 0}, {'text': 'how does one specify the syntactical restrictions', 'label': 1}, {'text': 'what are the structure or functions of congress', 'label': 0}, {'text': 'make a radio announcement that advertise the book write it out', 'label': 2}, {'text': 'calculate the number of sacrificial anodes that would be required to form a calcareous deposit on a steel pile immersed in seawater ', 'label': 1}, {'text': 'can you apply the method used to some experience of your own ', 'label': 1}, {'text': 'summarize a story in own words', 'label': 0}, {'text': 'what is your family name', 'label': 0}, {'text': 'how many ways can you ', 'label': 2}, {'text': 'draw and label a diagram of a typical stream', 'label': 0}, {'text': 'how did chinese newspapers differ from us', 'label': 2}, {'text': 'classify animals into two groups', 'label': 0}, {'text': 'after examining the videotape of a play in a football game determine the degree to which the defensive team performed effectively and', 'label': 2}, {'text': 'judge whether it would be possible to survive on an island alone and blinded write about it', 'label': 2}, {'text': 'in what ways do female offenders differ from male offenders', 'label': 2}, {'text': 'given two possible solutions a and b to solving the given software development problem decide on the best solution give your justification', 'label': 2}, {'text': 'how do teenagers use code in emails and instant messages', 'label': 1}, {'text': 'given these five corrosion inhibitors v w x y z identify which are passivators ', 'label': 0}, {'text': 'state the formula for the area of a circle', 'label': 0}, {'text': 'complete an analogy analogy tasks are inference tasks', 'label': 0}, {'text': 'given the data available on a research question take a position and defend it', 'label': 2}, {'text': 'who was the first man on the moon', 'label': 0}, {'text': 'how many instructions in the instruction set of risc computer', 'label': 0}, {'text': 'what do you think about ', 'label': 2}, {'text': 'differentiate between the following terms along with appropriate examples', 'label': 2}, {'text': 'can you distinguish between ', 'label': 2}, {'text': 'justify and nominate ways to prevent animal extinction', 'label': 2}, {'text': 'determine if a characters actions were heroic ', 'label': 2}, {'text': 'explain the process paraphrase for finding the perimeter of a rectangular garden', 'label': 0}, {'text': 'construct a family of networks to demonstrate that the number of different mincuts in a network can be exponential in the size of the network', 'label': 1}, {'text': 'in which part of the world does he live ', 'label': 0}, {'text': 'how could you rewrite this story with a city setting', 'label': 2}, {'text': 'write a poem about this book', 'label': 2}, {'text': 'classify celebrations into family and community categories', 'label': 0}, {'text': 'state the guidelines to design an anodic protection system', 'label': 0}, {'text': 'use laplace to solve the following problems', 'label': 1}, {'text': 'assess the strengths and weaknesses of the current olympics and recommend action that should be taken in future olympics what can be', 'label': 2}, {'text': 'paraphrase this poem about sheik schubli explaining what he was saying about the characteristics of a friend', 'label': 0}, {'text': 'develop a sqa plan for a software development project which is defined in the attached document', 'label': 2}, {'text': 'compose a rhythm or put new words to a known melody', 'label': 2}, {'text': 'list reserved words in c programming', 'label': 0}, {'text': 'name steps that follow the taking of notes within hours or less', 'label': 0}, {'text': 'how would you distinguish between polymyositis and viral myositis in a yearold man with weakness and a rash', 'label': 2}, {'text': 'summarize a historical document', 'label': 0}, {'text': 'decide about the most exciting part of the book being sure to give at least three reasons why', 'label': 2}, {'text': 'how would you feel if ', 'label': 2}, {'text': 'develop and execute a program for this exercise', 'label': 2}, {'text': 'in one sentence illustrate the main point of a written passage', 'label': 0}, {'text': 'how would you restructure the school day to reflect childrens developmental needs', 'label': 2}, {'text': 'if you had access to all resources how would you deal with ', 'label': 2}, {'text': 'compare herbatious and carnivorous animals on a venn diagram', 'label': 2}, {'text': 'determine the authors point of view', 'label': 0}, {'text': 'identify fractions from a pictorial representation', 'label': 0}, {'text': 'compose your own notetaking design that incorporates all theadvantages of the cornell method', 'label': 2}, {'text': 'can you develop a proposal which would ', 'label': 2}, {'text': 'decide which candidate would best fill the position of principal', 'label': 2}, {'text': 'what ideas can you add to', 'label': 2}, {'text': 'compose a class story', 'label': 2}, {'text': 'make a diorama to illustrate an important event', 'label': 1}, {'text': 'how would you differentiate obstructive apneas from central apneas', 'label': 2}, {'text': 'explain choices made in making recommendations to an end user', 'label': 2}, {'text': 'design a machine to perform a specific task', 'label': 2}, {'text': 'name the artist who painted the mona lisa', 'label': 0}, {'text': 'how would you show your understanding of the abc process', 'label': 1}, {'text': 'compose music for a frog play', 'label': 2}, {'text': 'can you invent another character for the story', 'label': 2}, {'text': 'construct one turing machine for computing each of the following functions', 'label': 1}, {'text': 'what would you predictinfer from', 'label': 2}, {'text': 'can you explain what must have happened when ', 'label': 2}, {'text': 'design a cost effective strategy to generate reliable data', 'label': 2}, {'text': 'apply laws of statistics to evaluate the reliability of a written test', 'label': 1}, {'text': 'construct the bit ripple carry adder circuit', 'label': 1}, {'text': 'decide whether you could have survived on the island blind and alone write about things that would have been challenging making sure to', 'label': 2}, {'text': 'given the two solutions to the stated programming problem rate the solutions in terms of efficiency and readability', 'label': 2}, {'text': 'analyze the movements and sounds of a frog', 'label': 2}, {'text': 'choose a paint schedule for the maintenance of an outdoor structure level ', 'label': 2}, {'text': 'relate your most powerful story where your use of technology in the classroom has yielded a significant outcome with a student or students', 'label': 1}, {'text': 'recommend how our classroom or playground could be improved', 'label': 2}, {'text': 'apply your understanding the olympic spirit to develop a new motto or slogan', 'label': 1}, {'text': 'create a set of guidelines to determine the points of a plant susceptible to localized corrosion', 'label': 2}, {'text': 'develop a hypothesis', 'label': 2}, {'text': 'decide which course of action was most effective ', 'label': 2}, {'text': 'using the basic principles of socialism discussed in this course evaluate the us economic system by providing key arguments to support your judgment', 'label': 2}, {'text': 'in one sentence explain the main idea of a written passage', 'label': 0}, {'text': 'is there a better solution to ', 'label': 2}, {'text': 'describe how phillip and timothy survived on the cay', 'label': 0}, {'text': 'illustrate what you think the main idea was', 'label': 0}, {'text': 'how do we differentiate between genuine buyer and undercover buyer', 'label': 2}, {'text': 'list four types of clauses mentioned in the article', 'label': 0}, {'text': 'in what ways trajectory patterns differ according to abc', 'label': 2}, {'text': 'describe the major clinical differences between visceral and somatic pain', 'label': 0}, {'text': 'how much data can be stored in the cach memory', 'label': 0}, {'text': 'list the characteristics peculiar to the cubist movement', 'label': 0}, {'text': 'outline the most important insight of the tale', 'label': 0}, {'text': 'analyze the positive and negative points presented concerning the abolition of guns and write a brief page narrative of your analysis', 'label': 2}, {'text': 'construct an analog circuit with the following specifications', 'label': 1}, {'text': 'inspect each of these boolean expressions and determine whether each one is a sum of products or a product of sums', 'label': 2}, {'text': 'examine the data in the employees and departments tables', 'label': 2}, {'text': 'identify the five major prophets of the old testament', 'label': 0}, {'text': 'in a teaching simulation with your peers roleplaying th grade students demonstrate the principle of reinforcement in classroom interactions and prepare a half page description of what happened during the simulation that validated the principle', 'label': 1}, {'text': 'explain what the author means by the statement our intellectual powers are geared to master static relations', 'label': 0}, {'text': 'suppose phillip wasnt rescued shortly after timothys death how long could he have survived', 'label': 2}, {'text': 'show me how you would check the brake lights are working on this car', 'label': 1}, {'text': 'describe in your own words how to copy text from one program into another', 'label': 0}, {'text': 'develop a network based on the following information activity immediate predecessors', 'label': 2}, {'text': 'summarize the basic tenets of collaborative conservation', 'label': 0}, {'text': 'outline the mechanisms employed by bacteria allowing them to evade phagocytic destruction', 'label': 0}, {'text': 'examine the stated positions of both major political candidates with regard to a particular issue and state good reasons based on principles discussed in class for why one candidates position is more likely to be effective than the others', 'label': 2}, {'text': 'can you defend the idea that simons incident with the pigs head is the most mystical in the story', 'label': 2}, {'text': 'distinguish between micro and macro economics', 'label': 2}, {'text': 'calculate the rate of habitat fragmentation within the colorado front range in the last decade', 'label': 1}, {'text': 'explain in ones own words how to create a query in a database', 'label': 0}, {'text': 'outline an attack plan that will allow you to test your hypothesis using computational molecular modeling tools', 'label': 0}, {'text': 'derive a kinetic model from experimental data', 'label': 1}, {'text': 'use a manual to calculate an employees vacation time', 'label': 1}, {'text': 'how can we construct h given i b and e', 'label': 1}, {'text': 'compose a device that would assist an athlete in their training', 'label': 2}, {'text': 'use the eph or pourbaix diagram of cr to determine the control current and potential for the anodic protection of a s stainless steel vessel ', 'label': 0}, {'text': 'when did you born ', 'label': 0}, {'text': 'compare two dog food commercials what is the difference between them and how do they both sell their products', 'label': 2}, {'text': 'explain what a poem means', 'label': 0}, {'text': 'after designing an experiment examining the results and drawing conclusions determines in what ways the experiment could be conducted more effectively in order to draw more productive conclusions in the future', 'label': 2}, {'text': 'define four types of traceability', 'label': 0}, {'text': 'can you defend your position about ', 'label': 2}, {'text': 'do you think is a good or a bad thing', 'label': 2}, {'text': 'estimate roughly how long the collision process takes', 'label': 2}, {'text': 'sketch a typical absorption process in a packed column with regeneration in form of', 'label': 1}, {'text': 'explain how timothy saved phillips life', 'label': 0}, {'text': 'what do you think is the mass and why', 'label': 2}, {'text': 'summarize jems beliefs about boo radley', 'label': 0}, {'text': 'appraise data in support of a hypothesis', 'label': 2}, {'text': 'explain why it is likely that a matriarchal family system would be found in a matrilocal or matrilineal society', 'label': 2}, {'text': 'do you agree', 'label': 2}, {'text': 'inspect the size of the loaded data how many objects and variables does the iris data matrix contain', 'label': 2}, {'text': 'design a new animal to live in the jungle', 'label': 2}, {'text': 'investigate pointwise convergence and absolute convergence of the given series of functions', 'label': 2}, {'text': 'can you create new and unusual uses for ', 'label': 2}, {'text': 'how would you createdesign a new', 'label': 2}, {'text': 'summarize the main point of these two texts in your own words ', 'label': 0}, {'text': 'state in your own words what the author means when he says', 'label': 0}, {'text': 'outline your experience in working with service users', 'label': 0}, {'text': 'are you a person', 'label': 2}, {'text': 'pretend you are one of the characters in the book write a diary about the happenings in your life for two consecutive days', 'label': 1}, {'text': 'sketch the blackbody curves for radiation emitted from the sun and the earth', 'label': 1}, {'text': 'use gauss law to find the electric field between the plates when the charge on them is q', 'label': 1}, {'text': 'construct a dictionary and a truth table to determine if the following statements are logically equivalent', 'label': 1}, {'text': 'what would result if ', 'label': 1}, {'text': 'can you distinguish for me between what is reasonably achievable and what are exaggerations', 'label': 2}, {'text': 'in your own words how would you define transferable skills', 'label': 0}, {'text': 'translate a written text aloud from l to english', 'label': 0}, {'text': 'do you devise a systematic approach in order to get larger progressions', 'label': 2}, {'text': 'use a judge and jury to discuss the statement children enjoy anthony brownes books because of the illustrations', 'label': 2}, {'text': 'outline in your own words how the leggos tomato paste advertisement sells their product', 'label': 0}, {'text': 'devise a new economic system based on the ones that already exist', 'label': 2}, {'text': 'how was life different in your town years ago', 'label': 2}, {'text': 'design a testing scenario to assess the susceptibility of an alloy to be used in a given environment', 'label': 2}, {'text': 'take a collection of photographs to demonstrate a particular point', 'label': 1}, {'text': 'model an olympic village for the new millenium', 'label': 1}, {'text': 'draw a picture of the bears house', 'label': 1}, {'text': 'how to specify the impact energy and impact test temperature of the material in the code', 'label': 1}, {'text': 'use the secondderivative test to determine whether critical points where fx yield relative maxima or relative minima', 'label': 1}, {'text': 'draw pictures showing the beginning middle and ending of the story', 'label': 1}, {'text': 'restate the olympic motto in your own words', 'label': 0}, {'text': 'how to construct simple solutions when problems are complex', 'label': 1}])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nu_data['train'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../preprocess/arg_split.json\") as f:\n",
    "    arg_data = json.load(f)\n",
    "\n",
    "arg_tokenized_text = tokenize_all_text(embed_lookup, arg_data['train'])\n",
    "arg_test_tokenized_text = tokenize_all_text(embed_lookup, arg_data['test'])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "arg_train_features = pad_features(arg_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(arg_train_features)==len(arg_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(arg_train_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(arg_train_features[:20,:8])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "arg_test_features = pad_features(arg_test_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(arg_test_features)==len(arg_test_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(arg_test_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(arg_test_features[:20,:8])\n",
    "\n",
    "\n",
    "arg_train_labels = np.array([item['label'] for item in arg_data['train'].values()])\n",
    "arg_test_labels = np.array([item['label'] for item in arg_data['test'].values()])\n",
    "\n",
    "print(arg_test_labels[:20])\n",
    "\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(arg_train_features)*split_frac)\n",
    "arg_train_x, arg_valid_x = arg_train_features[:split_idx], arg_train_features[split_idx:]\n",
    "arg_train_y, arg_valid_y = arg_train_labels[:split_idx], arg_train_labels[split_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(arg_train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(arg_valid_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(arg_test_features.shape))\n",
    "\n",
    "# create Tensor datasets\n",
    "arg_train_data = TensorDataset(torch.from_numpy(arg_train_x), torch.from_numpy(arg_train_y))\n",
    "arg_valid_data = TensorDataset(torch.from_numpy(arg_valid_x), torch.from_numpy(arg_valid_y))\n",
    "arg_test_data = TensorDataset(torch.from_numpy(arg_test_features), torch.from_numpy(arg_test_labels))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 4\n",
    "\n",
    "# shuffling and batching data\n",
    "arg_train_loader = DataLoader(arg_train_data, shuffle=True, batch_size=batch_size)\n",
    "arg_valid_loader = DataLoader(arg_valid_data, shuffle=True, batch_size=batch_size)\n",
    "arg_test_loader = DataLoader(arg_test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "# print some example review/sentiment text\n",
    "print(reviews[:1000])\n",
    "print()\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Pre-processing\n",
    "\n",
    "The first step, when building a neural network, is getting the data into the proper form to feed into the network. Since I'm planning to use a word-embedding layer, I know that I'll need to encode each word in a reviews as an integer, and encode each sentiment label as 1 (positive) or 0 (negative). \n",
    "\n",
    "I'll first want to clean up the reviews by removing punctuation and converting them to lowercase. You can see an example of the reviews data, above. Here are the processing steps, I'll want to take:\n",
    ">* Get rid of any extraneous punctuation.\n",
    "* You might notice that the reviews are delimited with newline characters `\\n`. To deal with those, I'm going to split the text into each review using `\\n` as the delimiter. \n",
    "* Then I can combined all the reviews back together into one big string to get all of my text data.\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of all words\n",
    "all_words = all_text.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Labels\n",
    "\n",
    "The review labels are \"positive\" or \"negative\". To use these labels in a neural network, I need to convert them to numerical values, 1 (positive) and 0 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers\n",
    "\n",
    "As an additional pre-processing step, I want to make sure that the reviews are in good shape for standard processing. That is, I'll want to shape the reviews into a specific, consistent length for ease of processing and comparison. I'll approach this task in two main steps:\n",
    "\n",
    "1. Getting rid of extremely long or short reviews; the outliers\n",
    "2. Padding/truncating the remaining data so that we have reviews of the same length.\n",
    "\n",
    "Before I pad the review text, below, I am checking for reviews of extremely short or long lengths; outliers that may mess with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build a dictionary that maps indices to review lengths\n",
    "counts = Counter(all_words)\n",
    "\n",
    "# outlier review stats\n",
    "# counting words in each review\n",
    "review_lens = Counter([len(x.split()) for x in reviews_split])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, a couple issues here. I seem to have one review with zero length. And, the maximum review length is really long. I'm going to remove any super short reviews and truncate super long reviews. This removes outliers and should allow our model to train more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_split))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using a Pre-Trained Embedding Layer\n",
    "\n",
    "Next, I'll want to tokenize my reviews; turning the list of words that make up a given review into a list of tokenized integers that represent those words. Typically, this is done by creating a dictionary that maps each unique word in a vocabulary to a specific integer value.\n",
    "\n",
    "In this example, I'll actually want to use a mapping that already exists, in a pre-trained embedding layer. Below, I am loading in a pre-trained embedding model, and I'll explore its traits.\n",
    "\n",
    "> This code assumes I have a downloaded model `GoogleNews-vectors-negative300-SLIM.bin.gz` in the same directory as this notebook, in a folder, `word2vec_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained word2vec model (only need to run code, once)\n",
    "# ! gzip -d word2vec_model/GoogleNews-vectors-negative300-SLIM.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Word2Vec loading capabilities\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('word2vec_model/GoogleNews-vectors-negative300-SLIM.bin', \n",
    "                                                 binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "You can think of an embedding layer as a lookup table, where the rows are indexed by word token and the columns hold the embedding values. For example, row 958 is the embedding vector for the word that maps to the integer value 958.\n",
    "\n",
    "<img src='notebook_ims/embedding_lookup_table.png' width=40% />\n",
    "\n",
    "In the below cells, I am storing the words in the pre-trained vocabulary, and printing out the size of the vocabulary and word embeddings. \n",
    "> The embedding dimension from the pret-rained model is 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'for', 'that', 'is', 'on']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed_lookup.index_to_key:\n",
    "    pretrained_words.append(word)\n",
    "pretrained_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 299567\n",
      "\n",
      "Word in vocab: for\n",
      "\n",
      "Length of embedding: 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_idx = 1\n",
    "\n",
    "# get word/embedding in that row\n",
    "word = pretrained_words[row_idx] # get words by index\n",
    "embedding = embed_lookup[word] # embeddings by word\n",
    "\n",
    "# vocab and embedding info\n",
    "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
    "print('Word in vocab: {}\\n'.format(word))\n",
    "print('Length of embedding: {}\\n'.format(len(embedding)))\n",
    "#print('Associated embedding: \\n', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "for\n",
      "that\n",
      "is\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "# print a few common words\n",
    "for i in range(5):\n",
    "    print(pretrained_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "The pre-trained embedding model has learned to represent semantic relationships between words in vector space. Specifically, words that appear in similar contexts should point in roughly the same direction. To measure whether two vectors are colinear, we can use [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity), which computes the dot product of two vectors. This dot product is largest when the angle between two vectors is 0 (cos(0) = 1) and cosine is at a maximum, so cosine similarity is larger for aligned vectors.\n",
    "\n",
    "<img src='notebook_ims/two_vectors.png' width=30% />\n",
    "\n",
    "### Embedded Bias\n",
    "\n",
    "Word2Vec, in addition to learning useful similarities and semantic relationships between words, also learns to represent problematic relationships between words. For example, a paper on [Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf) by Bolukbasi et al. (2016), found that the vector-relationship between \"man\" and \"woman\" was similar to the relationship between \"physician\" and \"registered nurse\" or \"shopkeeper\" and \"housewife\" in the trained, Google News Word2Vec model, **which I am using in this notebook**.\n",
    "\n",
    ">*\"In this paper, we quantitatively demonstrate that word-embeddings contain biases in their geometry that reflect gender stereotypes present in broader society. Due to their wide-spread usage as basic\n",
    "features, word embeddings not only reflect such stereotypes but can also amplify them. This poses a\n",
    "significant risk and challenge for machine learning and its applications.\"*\n",
    "\n",
    "As such, it is important to note that this example is using a Word2Vec model that has been shown to encapsulate gender stereotypes.\n",
    "\n",
    "You can explore similarities and relationships between word embeddings using code. The code below finds words with the highest cosine similarity when compared to the word `find_similar_to`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to fabulous: \n",
      "\n",
      "Word: wonderful, Similarity: 0.761\n",
      "Word: fantastic, Similarity: 0.761\n",
      "Word: marvelous, Similarity: 0.730\n",
      "Word: gorgeous, Similarity: 0.714\n",
      "Word: lovely, Similarity: 0.713\n",
      "Word: terrific, Similarity: 0.694\n",
      "Word: amazing, Similarity: 0.693\n",
      "Word: beautiful, Similarity: 0.670\n",
      "Word: magnificent, Similarity: 0.667\n",
      "Word: splendid, Similarity: 0.645\n"
     ]
    }
   ],
   "source": [
    "# Pick a word \n",
    "find_similar_to = 'fabulous'\n",
    "\n",
    "print('Similar words to '+find_similar_to+': \\n')\n",
    "\n",
    "# Find similar words, using cosine similarity\n",
    "# by default shows top 10 similar words\n",
    "for similar_word in embed_lookup.similar_by_word(find_similar_to):\n",
    "    print(\"Word: {0}, Similarity: {1:.3f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize reviews\n",
    "\n",
    "The pre-trained embedding layer already has tokens associated with each word in the dictionary. I want to use that same mapping to tokenize all the reviews in the movie review corpus. I will encode any unknown words (words that appear in the reviews but not in the pre-trained vocabulary) as the whitespace token, 0; this should be fine for the purpose of sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews to tokens\n",
    "def tokenize_all_text(embed_lookup, data):\n",
    "    # split each review into a list of words\n",
    "    words = [item['text'].split() for item in data.values()]\n",
    "\n",
    "    tokenized_text = []\n",
    "    for text in words:\n",
    "        ints = []\n",
    "        for word in text:\n",
    "            try:\n",
    "                idx = embed_lookup.key_to_index[word]\n",
    "            except: \n",
    "                idx = 0\n",
    "            ints.append(idx)\n",
    "        tokenized_text.append(ints)\n",
    "    \n",
    "    return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenize_all_text(embed_lookup, data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_text = tokenize_all_text(embed_lookup, data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_tokenized_text = tokenize_all_text(embed_lookup, nu_data['train'])\n",
    "nu_test_tokenized_text = tokenize_all_text(embed_lookup, nu_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[780, 0, 73009, 1, 0, 9560, 0, 116, 798, 192, 9, 194, 3206]\n"
     ]
    }
   ],
   "source": [
    "# testing code and printing a tokenized review\n",
    "print(nu_tokenized_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, I'll pad or truncate all the reviews to a specific length. For reviews shorter than some `seq_length`, I'll left-pad with 0s. For reviews longer than `seq_length`, I'll truncate them to the first `seq_length` words. A good `seq_length`, in this case, is about 200.\n",
    "\n",
    "> The function `pad_features` returns an array that contains padded, tokenized reviews, of a standard size, that we'll pass to the network. \n",
    "\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input, tokenized review is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**\n",
    "\n",
    "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tokenized_text, seq_length):\n",
    "    ''' Return features of tokenized_reviews, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tokenized_text), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(tokenized_text):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0    223     49  11534      0      0   4338   5408]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0    897    219      9 130013   1676      4      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0    223     49  11534      0      0   4338   5408]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "train_features = pad_features(tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(train_features)==len(tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(train_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(train_features[:20,:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "test_features = pad_features(test_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(test_features)==len(test_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(test_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(test_features[:20,:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0   780     0 73009     1     0  9560]\n",
      " [    0     0     0  4699  4480     9  4516  1207]\n",
      " [    0     0     0     0     0     0     0  9660]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0  9660     9 19809     0     0]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0    78     3     9]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0    56]\n",
      " [    0     0     0     0  4328 86996  2629     0]\n",
      " [    0     0     0     0     0     0  4196     9]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0  7460]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0 23690     9   459]\n",
      " [  132    53    38  5838     9   389     0  1031]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "[[     0      0      0  12646     25  24884  86996     12]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0   4480      9]\n",
      " [     0      0      0      0      0      0    132    171]\n",
      " [     0    132     42     25    521     14    373     83]\n",
      " [     0      0      0      0      0    455     49   6009]\n",
      " [     0      0      0      0      0    132     42     38]\n",
      " [     0      0      0      0   1715      0   1891      0]\n",
      " [     0    116    235   1146    856     78      3   1982]\n",
      " [     0   5141      9    390      0 138536  32197    160]\n",
      " [     0      0      0      0      0    132      0  11666]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0    856    132]\n",
      " [     0      0    455   3405    737   3704      9    100]\n",
      " [     0      0      0      0      0      0      0   5812]\n",
      " [     0      0      0      0      0    132    141     54]\n",
      " [     0      0      0      0      0   5838     95     49]\n",
      " [     0      0   2873      9    917   2308      9   1915]\n",
      " [     0      0      0      0    680      9   4370      0]\n",
      " [     0      0      0      0      0      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "nu_train_features = pad_features(nu_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(train_features)==len(nu_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(train_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(train_features[:20,:8])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "nu_test_features = pad_features(nu_test_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(test_features)==len(nu_test_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(test_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(test_features[:20,:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training, Validation, and Test Data\n",
    "\n",
    "With the data in nice shape, I'll split it into training, validation, and test sets.\n",
    "\n",
    "In the below code, I am creating features (x) and labels (y). \n",
    "* The split fraction, `split_frac` defines the fraction of data to **keep** in the training set. Usually this is set to 0.8 or 0.9. \n",
    "* Whatever data is left is split in half to create the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([item['label'] for item in data['train'].values()])\n",
    "test_labels = np.array([item['label'] for item in data['test'].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NU\n",
    "nu_train_labels = np.array([item['label'] for item in nu_data['train'].values()])\n",
    "nu_test_labels = np.array([item['label'] for item in nu_data['test'].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nu_train_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(train_features)*split_frac)\n",
    "train_x, valid_x = train_features[:split_idx], train_features[split_idx:]\n",
    "train_y, valid_y = train_labels[:split_idx], train_labels[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NU\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(nu_train_features)*split_frac)\n",
    "nu_train_x, nu_valid_x = nu_train_features[:split_idx], nu_train_features[split_idx:]\n",
    "nu_train_y, nu_valid_y = nu_train_labels[:split_idx], nu_train_labels[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(333, 15) \n",
      "Validation set: \t(84, 15) \n",
      "Test set: \t\t(179, 15)\n"
     ]
    }
   ],
   "source": [
    "# split_frac = 0.8\n",
    "\n",
    "# ## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "# split_idx = int(len(features)*split_frac)\n",
    "# train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "# train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "# test_idx = int(len(remaining_x)*0.5)\n",
    "# val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "# val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(nu_train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(nu_valid_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(nu_test_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your work**\n",
    "\n",
    "With train, validation, and test fractions equal to 0.8, 0.1, 0.1, respectively, the final, feature data shapes should look like:\n",
    "```\n",
    "                    Feature Shapes:\n",
    "Train set: \t\t (20000, 200) \n",
    "Validation set: \t(2500, 200) \n",
    "Test set: \t\t  (2500, 200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, I can create DataLoaders for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_features), torch.from_numpy(test_labels))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 4\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NU\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "nu_train_data = TensorDataset(torch.from_numpy(nu_train_x), torch.from_numpy(nu_train_y))\n",
    "nu_valid_data = TensorDataset(torch.from_numpy(nu_valid_x), torch.from_numpy(nu_valid_y))\n",
    "nu_test_data = TensorDataset(torch.from_numpy(nu_test_features), torch.from_numpy(nu_test_labels))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 4\n",
    "\n",
    "# shuffling and batching data\n",
    "nu_train_loader = DataLoader(nu_train_data, shuffle=True, batch_size=batch_size)\n",
    "nu_valid_loader = DataLoader(nu_valid_data, shuffle=True, batch_size=batch_size)\n",
    "nu_test_loader = DataLoader(nu_test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   986      9   2887     12    390  58549     22     37]\n",
      " [     0     25  22694      9    770      0     78  72206]\n",
      " [     0  15370  10297  15760   1504      0   1444      0]\n",
      " [     9  24325   1947      8   2041      0      9    271]\n",
      " [     9   6398      2   6837     78   1599      0    192]\n",
      " [  5097      0   4408      0   1015      0  11542   3677]\n",
      " [    40   4932      0   1218   2000     22     37      2]\n",
      " [    34    857      0      9   4588      0      0  13080]\n",
      " [  4818     35   1471   5695      0   9577  21774      9]\n",
      " [    66   3411      3      0   8561      0 231574    179]\n",
      " [    34   2093   3980      0   7712      0  60560  10492]\n",
      " [    25   1427   2000  22915 104856 231574      1  36464]\n",
      " [  1682     24   7558   7110      0    129      0    125]\n",
      " [     0      0      0     34   4094      0     60   5426]\n",
      " [     0      0      9   8190    483     16      9   1931]\n",
      " [     0    957      9   8521   3936      0  80045  37286]\n",
      " [     0   5526   2878   3106    160   3079      0   2108]\n",
      " [  2975     34     17   1528      0    132   1206   3475]\n",
      " [   986      0     22     37  37123   3199      0   9385]\n",
      " [     0    142  10176 112503    676    554      0     78]]\n",
      "[[    67     17     49    488  10150      0   1949   9666]\n",
      " [     9    655   3383      0  34490 147937   3988     27]\n",
      " [     0      0     34   4094      9   1300  17705      1]\n",
      " [     1      9      0  30269      9   1252  12674   9174]\n",
      " [  1379   6717  11660     17   1409      0   7580   3422]\n",
      " [  6581      2     34   1366     73   5695   2956   6610]\n",
      " [     0      0      0  13223     16  15455   1613    130]\n",
      " [    10  28039   4317    997  80045  85080      0 134972]\n",
      " [     1   2411   2313    124    554  26267     82      4]\n",
      " [     0      0      0      0   1408      0     25  27492]\n",
      " [     9      0   9027   8699      0      9   4173      2]\n",
      " [   664      9   3917    297      1   3260      9    680]\n",
      " [     0   1161      1      9    389      0  15370  10297]\n",
      " [     0      0      0      0      0   1553   1207      3]\n",
      " [     0      0      0    681   5843     14      9    189]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [   213      9  26928      0      0   5395      3   9215]\n",
      " [    66    606    819      9  13064   2135  12987      0]\n",
      " [  5725    365      9      0   3420   1329  15760      9]\n",
      " [     0      9     49   9642     17    841      0   3277]]\n",
      "[0 3 3 0 0 3 0 3 0 0 0 2 0 0 0 0 0 1 0 3]\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(703, 15) \n",
      "Validation set: \t(176, 15) \n",
      "Test set: \t\t(378, 15)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../preprocess/arg_split.json\") as f:\n",
    "    arg_data = json.load(f)\n",
    "\n",
    "arg_tokenized_text = tokenize_all_text(embed_lookup, arg_data['train'])\n",
    "arg_test_tokenized_text = tokenize_all_text(embed_lookup, arg_data['test'])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "arg_train_features = pad_features(arg_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(arg_train_features)==len(arg_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(arg_train_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(arg_train_features[:20,:8])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "arg_test_features = pad_features(arg_test_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(arg_test_features)==len(arg_test_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(arg_test_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(arg_test_features[:20,:8])\n",
    "\n",
    "\n",
    "arg_train_labels = np.array([item['label'] for item in arg_data['train'].values()])\n",
    "arg_test_labels = np.array([item['label'] for item in arg_data['test'].values()])\n",
    "\n",
    "print(arg_test_labels[:20])\n",
    "\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(arg_train_features)*split_frac)\n",
    "arg_train_x, arg_valid_x = arg_train_features[:split_idx], arg_train_features[split_idx:]\n",
    "arg_train_y, arg_valid_y = arg_train_labels[:split_idx], arg_train_labels[split_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(arg_train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(arg_valid_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(arg_test_features.shape))\n",
    "\n",
    "# create Tensor datasets\n",
    "arg_train_data = TensorDataset(torch.from_numpy(arg_train_x), torch.from_numpy(arg_train_y))\n",
    "arg_valid_data = TensorDataset(torch.from_numpy(arg_valid_x), torch.from_numpy(arg_valid_y))\n",
    "arg_test_data = TensorDataset(torch.from_numpy(arg_test_features), torch.from_numpy(arg_test_labels))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 4\n",
    "\n",
    "# shuffling and batching data\n",
    "arg_train_loader = DataLoader(arg_train_data, shuffle=True, batch_size=batch_size)\n",
    "arg_valid_loader = DataLoader(arg_valid_data, shuffle=True, batch_size=batch_size)\n",
    "arg_test_loader = DataLoader(arg_test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0      0      0      0      0      0  31722     78]\n",
      " [     0      0      0      0      0    132      3      9]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [128719    814      0    134      4      9   2032   8748]\n",
      " [     0      0      0      0      0      0      0     78]\n",
      " [    83     34   6510      9   3042      0      9 137375]\n",
      " [   132    330     90     45     34   9593    545   8229]\n",
      " [    50   1748      9    708      4   1172  17670  77415]\n",
      " [     0      0      0      0      0      0      0   2873]\n",
      " [     0      0      0   1715   2962      0    245    116]\n",
      " [     0      0      0      0      0      0      0      0]\n",
      " [  4779     78     21   1090     56   4365   1183      9]\n",
      " [     0      0      0      0     78     83      9   6149]\n",
      " [   132    171      9   9519      0      9    527   5129]\n",
      " [   494     12    124   3887     12     38     45      2]\n",
      " [     0      0      0     78      3      9   1840      0]\n",
      " [  4779     78     21   1090      0      9  12761     56]\n",
      " [     0      0      3     25     24  33277      0     24]\n",
      " [     0      0    606      0    346    426  20392   7677]\n",
      " [     0      0      0      0      0      0      0     78]]\n",
      "[[    0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0    78    17     9   904]\n",
      " [    0     0     0     0     0     0    78  1293]\n",
      " [    0     0     0     0     0     0  2873    10]\n",
      " [ 9065     9  4209 18225    78  3933  3027     3]\n",
      " [  596  1285     9 46637    36     0   116  2102]\n",
      " [    0     0     0     0     0     0     0   111]\n",
      " [    0     0     0     0     0     0     0     0]\n",
      " [    0     9 22248     0  2131   713   512     3]\n",
      " [   56   545  2599     4     0  3413   982    24]\n",
      " [    0     0   990  1400     0  2900     0 31722]\n",
      " [   43    22     0   476  9519     0 12761     9]\n",
      " [    0     0     0     0     0    78    21    14]\n",
      " [  512    17  3302     0     9    51     0  6161]\n",
      " [    0     0     0     0     0     0     0   132]\n",
      " [    0     0     0   132   171     9   875     0]\n",
      " [ 2873    43   125     9  1595     0     9 36622]\n",
      " [ 2873     9  8886  2035    38   187     0    53]\n",
      " [    0   525     0    86 34825     0  8357     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "[0 1 0 2 0 1 1 0 1 0 1 1 1 1 0 1 1 2 1 0]\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(192, 15) \n",
      "Validation set: \t(48, 15) \n",
      "Test set: \t\t(104, 15)\n"
     ]
    }
   ],
   "source": [
    "# LREC\n",
    "import json\n",
    "with open(\"../preprocess/lrec_split.json\") as f:\n",
    "    lrec_data = json.load(f)\n",
    "\n",
    "lrec_tokenized_text = tokenize_all_text(embed_lookup, lrec_data['train'])\n",
    "lrec_test_tokenized_text = tokenize_all_text(embed_lookup, lrec_data['test'])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "lrec_train_features = pad_features(lrec_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(lrec_train_features)==len(lrec_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(lrec_train_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(lrec_train_features[:20,:8])\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "lrec_test_features = pad_features(lrec_test_tokenized_text, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(lrec_test_features)==len(lrec_test_tokenized_text), \"Features should have as many rows as reviews.\"\n",
    "assert len(lrec_test_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 8 values of the first 20 batches \n",
    "print(lrec_test_features[:20,:8])\n",
    "\n",
    "\n",
    "lrec_train_labels = np.array([item['label'] for item in lrec_data['train'].values()])\n",
    "lrec_test_labels = np.array([item['label'] for item in lrec_data['test'].values()])\n",
    "\n",
    "print(lrec_test_labels[:20])\n",
    "\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(lrec_train_features)*split_frac)\n",
    "lrec_train_x, lrec_valid_x = lrec_train_features[:split_idx], lrec_train_features[split_idx:]\n",
    "lrec_train_y, lrec_valid_y = lrec_train_labels[:split_idx], lrec_train_labels[split_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(lrec_train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(lrec_valid_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(lrec_test_features.shape))\n",
    "\n",
    "# create Tensor datasets\n",
    "lrec_train_data = TensorDataset(torch.from_numpy(lrec_train_x), torch.from_numpy(lrec_train_y))\n",
    "lrec_valid_data = TensorDataset(torch.from_numpy(lrec_valid_x), torch.from_numpy(lrec_valid_y))\n",
    "lrec_test_data = TensorDataset(torch.from_numpy(lrec_test_features), torch.from_numpy(lrec_test_labels))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 4\n",
    "\n",
    "# shuffling and batching data\n",
    "lrec_train_loader = DataLoader(lrec_train_data, shuffle=True, batch_size=batch_size)\n",
    "lrec_valid_loader = DataLoader(lrec_valid_data, shuffle=True, batch_size=batch_size)\n",
    "lrec_test_loader = DataLoader(lrec_test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Bi-LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiLSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, label_size, embedding_dim, hidden_dim, batch_size, seq_length, dropout=0.5, freeze_embeddings = True):\n",
    "        super(BiLSTMSentiment, self).__init__()\n",
    "        # set class vars\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 1. embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        self.embeddings.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embeddings.requires_grad = False\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(batch_first=True, input_size=embedding_dim, num_layers =2, hidden_size=hidden_dim, bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*2, label_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        x = self.embeddings(sentence)\n",
    "        _, (lstm_out, _) = self.lstm(x)\n",
    "        logits = self.hidden2label(lstm_out[-1])\n",
    "        # log_probs = F.log_softmax(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro F1: 0.439\n",
      "macro F1: 0.208\n",
      "weighted F1: 0.427\n",
      "\n",
      "micro precision: 0.439\n",
      "macro precision: 0.249\n",
      "weighted precision: 0.465\n",
      "\n",
      "micro recall: 0.439\n",
      "macro recall: 0.225\n",
      "weighted recall: 0.439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "# ARg\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "pred_tensor = None\n",
    "label_tensor = None\n",
    "\n",
    "\n",
    "arg_net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in arg_test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = arg_net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.argmax(output, dim=1)  # argmax\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    if pred_tensor == None:\n",
    "        pred_tensor = pred\n",
    "    else:\n",
    "        pred_tensor = torch.cat((pred_tensor, pred), dim=-1)\n",
    "\n",
    "    if label_tensor == None:\n",
    "        label_tensor = labels\n",
    "    else:\n",
    "        label_tensor = torch.cat((label_tensor, labels), dim=-1)\n",
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional import multiclass_precision\n",
    "from torcheval.metrics.functional import multiclass_recall\n",
    "\n",
    "print(\"micro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=4).item()))\n",
    "print(\"macro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=4, average='macro').item()))\n",
    "print(\"weighted F1: {:.3f}\\n\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=4, average='weighted').item()))\n",
    "\n",
    "print(\"micro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=4).item()))\n",
    "print(\"macro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=4, average='macro').item()))\n",
    "print(\"weighted precision: {:.3f}\\n\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=4, average='weighted').item()))\n",
    "\n",
    "print(\"micro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=4).item()))\n",
    "print(\"macro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=4, average='macro').item()))\n",
    "print(\"weighted recall: {:.3f}\\n\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=4, average='weighted').item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "The complete model is made of a few layers:\n",
    "\n",
    "**1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding)**\n",
    "* This converts our word tokens (integers) into embedded vectors of a specific size.\n",
    "* In this case, the vectors/weights of this layer will come from a **pretrained** lookup table. \n",
    "\n",
    "**2. A few [convolutional layers](https://pytorch.org/docs/stable/nn.html#conv1d)**\n",
    "* These are defined by an input size, number of filters/feature maps to output, and a kernel size.\n",
    "* The output of these layers will go through a ReLu activation function and pooling layer in the `forward` function.\n",
    "\n",
    "**3. A fully-connected, output layer**\n",
    "* This maps the convolutional layer outputs to a desired output_size (1 sentiment class).\n",
    "\n",
    "**4. A sigmoid activation layer**\n",
    "* This turns the output logit into a value 0-1; a class score.\n",
    "\n",
    "There is also a dropout layer, which will prevent overfitting, placed between the convolutional outputs and the final, fully-connected layer.\n",
    "\n",
    "<img src=\"notebook_ims/complete_embedding_CNN.png\" width=60%>\n",
    "\n",
    "*Image from the original paper, [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf).*\n",
    "\n",
    "### The Embedding Layer\n",
    "\n",
    "The embedding layer comes from our pre-trained `embed_lookup` model. By default, the weights of this layer are set to the vectors from the pre-trained model and frozen, so it will just be used as a lookup table. You could train your own embedding layer here, but it will speed up the training process to use a pre-trained model.\n",
    "\n",
    "### The Convolutional Layer(s)\n",
    "\n",
    "I am creating three convolutional layers, which will have kernel_sizes of (3, 300), (4, 300), and (5, 300); to look at 3-, 4-, and 5- sequences of word embeddings at a time. Each of these three layers will produce  100 filtered outputs. This is following the layer conventions in the paper, [CNNs for Sentence Classification](https://arxiv.org/abs/1408.5882).\n",
    "\n",
    "> The kernels only move in one dimension: down a sequence of word embeddings. In other words, these kernels move along a sequence of words, in time!\n",
    "\n",
    "### Maxpooling Layers\n",
    "\n",
    "In the `forward` function, I am applying a ReLu activation to the outputs of all convolutional layers and a maxpooling layer over the input sequence dimension. The maxpooling layer will get us an indication of whether some high-level text feature was found. \n",
    "\n",
    "> After moving through 3 convolutional layers with 100 filtered outputs each, these layers will output 300 values that can be sent to a final, fully-connected, classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
    "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # set class vars\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "        \n",
    "        # 2. convolutional layers\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2,0)) \n",
    "            for k in kernel_sizes])\n",
    "        \n",
    "        # 3. final, fully-connected layer for classification\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n",
    "        \n",
    "        # 4. dropout and sigmoid layers\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # self.sig = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        # conv_seq_length will be ~ 200\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        \n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        # embedded vectors\n",
    "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        \n",
    "        # get output of each conv-pool layer\n",
    "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
    "        \n",
    "        # concatenate results and add dropout\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # final logit\n",
    "        logit = self.fc(x) \n",
    "        \n",
    "        # raw logits for CE loss\n",
    "        return logit\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, I'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `num_filters`: Number of filters that each convolutional layer produces as output.\n",
    "* `filter_sizes`: A list of kernel sizes; one convolutional layer will be created for each kernel size.\n",
    "\n",
    "Any parameters I did not list, are left as the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 3 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMSentiment(\n",
      "  (embeddings): Embedding(299567, 300)\n",
      "  (lstm): LSTM(300, 128, bidirectional=True)\n",
      "  (hidden2label): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# arc lstm\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 3 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "arc_lstm = BiLSTMSentiment(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   hidden_dim, batch_size)\n",
    "\n",
    "print(arc_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "#NU\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 3 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "nu_net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(nu_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "# ARG\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 4 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "arg_net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(arg_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "# lrec\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 3 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "lrec_net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(lrec_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMSentiment(\n",
      "  (embeddings): Embedding(299567, 300)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (hidden2label): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "\n",
    "# lrec lstm\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 3 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "batch_size = 4\n",
    "seq_length = 15\n",
    "\n",
    "lrec_lstm = BiLSTMSentiment(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   hidden_dim, batch_size, seq_length)\n",
    "\n",
    "print(lrec_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Below is some training code, which iterates over all of the training data, records some loss statistics and performs backpropagation + optimization steps to update the weights of this network.\n",
    "\n",
    ">I'll also be using a binary cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.\n",
    "\n",
    "I also have some training hyperparameters:\n",
    "\n",
    "* `lr`: Learning rate for the optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "nu_optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(net, train_loader, epochs, print_every=100):\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    \n",
    "    # train for some number of epochs\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output, labels)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 1.121224... Val Loss: 1.093208\n",
      "Epoch: 1/5... Step: 20... Loss: 1.101917... Val Loss: 1.093345\n",
      "Epoch: 1/5... Step: 30... Loss: 1.097594... Val Loss: 1.093285\n",
      "Epoch: 1/5... Step: 40... Loss: 1.127493... Val Loss: 1.093037\n",
      "Epoch: 2/5... Step: 50... Loss: 1.111258... Val Loss: 1.092817\n",
      "Epoch: 2/5... Step: 60... Loss: 1.033905... Val Loss: 1.092382\n",
      "Epoch: 2/5... Step: 70... Loss: 1.074409... Val Loss: 1.091868\n",
      "Epoch: 2/5... Step: 80... Loss: 0.989854... Val Loss: 1.091301\n",
      "Epoch: 3/5... Step: 90... Loss: 1.064365... Val Loss: 1.090846\n",
      "Epoch: 3/5... Step: 100... Loss: 1.086898... Val Loss: 1.090428\n",
      "Epoch: 3/5... Step: 110... Loss: 1.144946... Val Loss: 1.090008\n",
      "Epoch: 3/5... Step: 120... Loss: 1.170827... Val Loss: 1.089649\n",
      "Epoch: 4/5... Step: 130... Loss: 1.039327... Val Loss: 1.089281\n",
      "Epoch: 4/5... Step: 140... Loss: 1.099073... Val Loss: 1.088922\n",
      "Epoch: 4/5... Step: 150... Loss: 1.140197... Val Loss: 1.088559\n",
      "Epoch: 4/5... Step: 160... Loss: 0.982836... Val Loss: 1.088184\n",
      "Epoch: 5/5... Step: 170... Loss: 1.070281... Val Loss: 1.087823\n",
      "Epoch: 5/5... Step: 180... Loss: 1.056839... Val Loss: 1.087476\n",
      "Epoch: 5/5... Step: 190... Loss: 1.011434... Val Loss: 1.087139\n",
      "Epoch: 5/5... Step: 200... Loss: 1.163367... Val Loss: 1.086797\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 5 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 10\n",
    "\n",
    "train(net, train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15... Step: 10... Loss: 1.119029... Val Loss: 1.076940\n",
      "Epoch: 1/15... Step: 20... Loss: 0.976671... Val Loss: 1.076753\n",
      "Epoch: 1/15... Step: 30... Loss: 1.172690... Val Loss: 1.076578\n",
      "Epoch: 1/15... Step: 40... Loss: 1.287001... Val Loss: 1.076412\n",
      "Epoch: 2/15... Step: 50... Loss: 1.029759... Val Loss: 1.076251\n",
      "Epoch: 2/15... Step: 60... Loss: 1.093583... Val Loss: 1.076088\n",
      "Epoch: 2/15... Step: 70... Loss: 1.196053... Val Loss: 1.075928\n",
      "Epoch: 2/15... Step: 80... Loss: 1.058536... Val Loss: 1.075770\n",
      "Epoch: 3/15... Step: 90... Loss: 1.221428... Val Loss: 1.075621\n",
      "Epoch: 3/15... Step: 100... Loss: 1.163355... Val Loss: 1.075482\n",
      "Epoch: 3/15... Step: 110... Loss: 1.364627... Val Loss: 1.075346\n",
      "Epoch: 3/15... Step: 120... Loss: 0.895639... Val Loss: 1.075215\n",
      "Epoch: 4/15... Step: 130... Loss: 1.091024... Val Loss: 1.075088\n",
      "Epoch: 4/15... Step: 140... Loss: 1.095452... Val Loss: 1.074970\n",
      "Epoch: 4/15... Step: 150... Loss: 1.069494... Val Loss: 1.074861\n",
      "Epoch: 4/15... Step: 160... Loss: 0.939687... Val Loss: 1.074756\n",
      "Epoch: 5/15... Step: 170... Loss: 1.201005... Val Loss: 1.074654\n",
      "Epoch: 5/15... Step: 180... Loss: 1.243943... Val Loss: 1.074556\n",
      "Epoch: 5/15... Step: 190... Loss: 1.090611... Val Loss: 1.074462\n",
      "Epoch: 5/15... Step: 200... Loss: 1.151527... Val Loss: 1.074372\n",
      "Epoch: 6/15... Step: 210... Loss: 0.968288... Val Loss: 1.074289\n",
      "Epoch: 6/15... Step: 220... Loss: 1.101202... Val Loss: 1.074205\n",
      "Epoch: 6/15... Step: 230... Loss: 1.176005... Val Loss: 1.074125\n",
      "Epoch: 6/15... Step: 240... Loss: 1.730017... Val Loss: 1.074049\n",
      "Epoch: 7/15... Step: 250... Loss: 0.943934... Val Loss: 1.073976\n",
      "Epoch: 7/15... Step: 260... Loss: 1.260500... Val Loss: 1.073918\n",
      "Epoch: 7/15... Step: 270... Loss: 1.043328... Val Loss: 1.073864\n",
      "Epoch: 7/15... Step: 280... Loss: 0.987928... Val Loss: 1.073813\n",
      "Epoch: 8/15... Step: 290... Loss: 1.042714... Val Loss: 1.073767\n",
      "Epoch: 8/15... Step: 300... Loss: 1.227913... Val Loss: 1.073724\n",
      "Epoch: 8/15... Step: 310... Loss: 1.108656... Val Loss: 1.073684\n",
      "Epoch: 8/15... Step: 320... Loss: 1.356588... Val Loss: 1.073656\n",
      "Epoch: 9/15... Step: 330... Loss: 1.109806... Val Loss: 1.073633\n",
      "Epoch: 9/15... Step: 340... Loss: 1.060648... Val Loss: 1.073616\n",
      "Epoch: 9/15... Step: 350... Loss: 1.165099... Val Loss: 1.073604\n",
      "Epoch: 9/15... Step: 360... Loss: 1.173676... Val Loss: 1.073597\n",
      "Epoch: 10/15... Step: 370... Loss: 1.262568... Val Loss: 1.073595\n",
      "Epoch: 10/15... Step: 380... Loss: 1.301097... Val Loss: 1.073602\n",
      "Epoch: 10/15... Step: 390... Loss: 1.021170... Val Loss: 1.073617\n",
      "Epoch: 10/15... Step: 400... Loss: 1.485005... Val Loss: 1.073636\n",
      "Epoch: 11/15... Step: 410... Loss: 1.178806... Val Loss: 1.073658\n",
      "Epoch: 11/15... Step: 420... Loss: 1.014792... Val Loss: 1.073690\n",
      "Epoch: 11/15... Step: 430... Loss: 0.970102... Val Loss: 1.073727\n",
      "Epoch: 11/15... Step: 440... Loss: 0.850833... Val Loss: 1.073769\n",
      "Epoch: 12/15... Step: 450... Loss: 0.830114... Val Loss: 1.073816\n",
      "Epoch: 12/15... Step: 460... Loss: 1.067506... Val Loss: 1.073867\n",
      "Epoch: 12/15... Step: 470... Loss: 1.430743... Val Loss: 1.073921\n",
      "Epoch: 12/15... Step: 480... Loss: 1.466905... Val Loss: 1.073977\n",
      "Epoch: 13/15... Step: 490... Loss: 0.978828... Val Loss: 1.074033\n",
      "Epoch: 13/15... Step: 500... Loss: 1.289982... Val Loss: 1.074095\n",
      "Epoch: 13/15... Step: 510... Loss: 0.930970... Val Loss: 1.074168\n",
      "Epoch: 13/15... Step: 520... Loss: 1.332920... Val Loss: 1.074246\n",
      "Epoch: 14/15... Step: 530... Loss: 1.073792... Val Loss: 1.074324\n",
      "Epoch: 14/15... Step: 540... Loss: 0.998000... Val Loss: 1.074406\n",
      "Epoch: 14/15... Step: 550... Loss: 1.288388... Val Loss: 1.074491\n",
      "Epoch: 14/15... Step: 560... Loss: 0.862698... Val Loss: 1.074578\n",
      "Epoch: 15/15... Step: 570... Loss: 1.031060... Val Loss: 1.074662\n",
      "Epoch: 15/15... Step: 580... Loss: 1.025133... Val Loss: 1.074746\n",
      "Epoch: 15/15... Step: 590... Loss: 0.908974... Val Loss: 1.074831\n",
      "Epoch: 15/15... Step: 600... Loss: 0.686475... Val Loss: 1.074918\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 15 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 10\n",
    "\n",
    "train(nu_net, train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 1.367653... Val Loss: 1.404443\n",
      "Epoch: 1/5... Step: 20... Loss: 1.342993... Val Loss: 1.403332\n",
      "Epoch: 1/5... Step: 30... Loss: 1.432281... Val Loss: 1.402293\n",
      "Epoch: 1/5... Step: 40... Loss: 1.332261... Val Loss: 1.401276\n",
      "Epoch: 1/5... Step: 50... Loss: 1.404521... Val Loss: 1.400265\n",
      "Epoch: 1/5... Step: 60... Loss: 1.459815... Val Loss: 1.399292\n",
      "Epoch: 1/5... Step: 70... Loss: 1.376378... Val Loss: 1.398344\n",
      "Epoch: 1/5... Step: 80... Loss: 1.439578... Val Loss: 1.397355\n",
      "Epoch: 1/5... Step: 90... Loss: 1.432746... Val Loss: 1.396285\n",
      "Epoch: 1/5... Step: 100... Loss: 1.471794... Val Loss: 1.395255\n",
      "Epoch: 1/5... Step: 110... Loss: 1.334440... Val Loss: 1.394261\n",
      "Epoch: 1/5... Step: 120... Loss: 1.521729... Val Loss: 1.393342\n",
      "Epoch: 1/5... Step: 130... Loss: 1.373954... Val Loss: 1.392389\n",
      "Epoch: 1/5... Step: 140... Loss: 1.453811... Val Loss: 1.391366\n",
      "Epoch: 1/5... Step: 150... Loss: 1.338229... Val Loss: 1.390343\n",
      "Epoch: 1/5... Step: 160... Loss: 1.364637... Val Loss: 1.389340\n",
      "Epoch: 1/5... Step: 170... Loss: 1.551330... Val Loss: 1.388363\n",
      "Epoch: 2/5... Step: 180... Loss: 1.302521... Val Loss: 1.387415\n",
      "Epoch: 2/5... Step: 190... Loss: 1.387928... Val Loss: 1.386554\n",
      "Epoch: 2/5... Step: 200... Loss: 1.486757... Val Loss: 1.385723\n",
      "Epoch: 2/5... Step: 210... Loss: 1.459797... Val Loss: 1.384960\n",
      "Epoch: 2/5... Step: 220... Loss: 1.444489... Val Loss: 1.384232\n",
      "Epoch: 2/5... Step: 230... Loss: 1.469884... Val Loss: 1.383478\n",
      "Epoch: 2/5... Step: 240... Loss: 1.310537... Val Loss: 1.382752\n",
      "Epoch: 2/5... Step: 250... Loss: 1.406371... Val Loss: 1.382057\n",
      "Epoch: 2/5... Step: 260... Loss: 1.321225... Val Loss: 1.381407\n",
      "Epoch: 2/5... Step: 270... Loss: 1.353008... Val Loss: 1.380787\n",
      "Epoch: 2/5... Step: 280... Loss: 1.346863... Val Loss: 1.380144\n",
      "Epoch: 2/5... Step: 290... Loss: 1.473360... Val Loss: 1.379553\n",
      "Epoch: 2/5... Step: 300... Loss: 1.368157... Val Loss: 1.378988\n",
      "Epoch: 2/5... Step: 310... Loss: 1.483541... Val Loss: 1.378454\n",
      "Epoch: 2/5... Step: 320... Loss: 1.404999... Val Loss: 1.377953\n",
      "Epoch: 2/5... Step: 330... Loss: 1.439822... Val Loss: 1.377466\n",
      "Epoch: 2/5... Step: 340... Loss: 1.302765... Val Loss: 1.376995\n",
      "Epoch: 2/5... Step: 350... Loss: 1.394175... Val Loss: 1.376524\n",
      "Epoch: 3/5... Step: 360... Loss: 1.392162... Val Loss: 1.376054\n",
      "Epoch: 3/5... Step: 370... Loss: 1.466998... Val Loss: 1.375631\n",
      "Epoch: 3/5... Step: 380... Loss: 1.396871... Val Loss: 1.375244\n",
      "Epoch: 3/5... Step: 390... Loss: 1.411070... Val Loss: 1.374876\n",
      "Epoch: 3/5... Step: 400... Loss: 1.343772... Val Loss: 1.374544\n",
      "Epoch: 3/5... Step: 410... Loss: 1.276285... Val Loss: 1.374267\n",
      "Epoch: 3/5... Step: 420... Loss: 1.323735... Val Loss: 1.374057\n",
      "Epoch: 3/5... Step: 430... Loss: 1.364323... Val Loss: 1.373889\n",
      "Epoch: 3/5... Step: 440... Loss: 1.379481... Val Loss: 1.373734\n",
      "Epoch: 3/5... Step: 450... Loss: 1.359722... Val Loss: 1.373624\n",
      "Epoch: 3/5... Step: 460... Loss: 1.394558... Val Loss: 1.373521\n",
      "Epoch: 3/5... Step: 470... Loss: 1.479337... Val Loss: 1.373437\n",
      "Epoch: 3/5... Step: 480... Loss: 1.256328... Val Loss: 1.373350\n",
      "Epoch: 3/5... Step: 490... Loss: 1.447545... Val Loss: 1.373260\n",
      "Epoch: 3/5... Step: 500... Loss: 1.359853... Val Loss: 1.373116\n",
      "Epoch: 3/5... Step: 510... Loss: 1.398004... Val Loss: 1.372984\n",
      "Epoch: 3/5... Step: 520... Loss: 1.348199... Val Loss: 1.372839\n",
      "Epoch: 4/5... Step: 530... Loss: 1.435632... Val Loss: 1.372705\n",
      "Epoch: 4/5... Step: 540... Loss: 1.357230... Val Loss: 1.372568\n",
      "Epoch: 4/5... Step: 550... Loss: 1.245624... Val Loss: 1.372416\n",
      "Epoch: 4/5... Step: 560... Loss: 1.493603... Val Loss: 1.372260\n",
      "Epoch: 4/5... Step: 570... Loss: 1.451288... Val Loss: 1.372098\n",
      "Epoch: 4/5... Step: 580... Loss: 1.357883... Val Loss: 1.371953\n",
      "Epoch: 4/5... Step: 590... Loss: 1.446481... Val Loss: 1.371807\n",
      "Epoch: 4/5... Step: 600... Loss: 1.432852... Val Loss: 1.371685\n",
      "Epoch: 4/5... Step: 610... Loss: 1.558411... Val Loss: 1.371585\n",
      "Epoch: 4/5... Step: 620... Loss: 1.391898... Val Loss: 1.371445\n",
      "Epoch: 4/5... Step: 630... Loss: 1.328416... Val Loss: 1.371308\n",
      "Epoch: 4/5... Step: 640... Loss: 1.270912... Val Loss: 1.371189\n",
      "Epoch: 4/5... Step: 650... Loss: 1.276505... Val Loss: 1.371051\n",
      "Epoch: 4/5... Step: 660... Loss: 1.562469... Val Loss: 1.370910\n",
      "Epoch: 4/5... Step: 670... Loss: 1.488674... Val Loss: 1.370779\n",
      "Epoch: 4/5... Step: 680... Loss: 1.395577... Val Loss: 1.370666\n",
      "Epoch: 4/5... Step: 690... Loss: 1.410947... Val Loss: 1.370556\n",
      "Epoch: 4/5... Step: 700... Loss: 1.335923... Val Loss: 1.370421\n",
      "Epoch: 5/5... Step: 710... Loss: 1.405047... Val Loss: 1.370284\n",
      "Epoch: 5/5... Step: 720... Loss: 1.459648... Val Loss: 1.370174\n",
      "Epoch: 5/5... Step: 730... Loss: 1.387676... Val Loss: 1.370112\n",
      "Epoch: 5/5... Step: 740... Loss: 1.394846... Val Loss: 1.370055\n",
      "Epoch: 5/5... Step: 750... Loss: 1.315501... Val Loss: 1.370051\n",
      "Epoch: 5/5... Step: 760... Loss: 1.481984... Val Loss: 1.370035\n",
      "Epoch: 5/5... Step: 770... Loss: 1.515281... Val Loss: 1.370048\n",
      "Epoch: 5/5... Step: 780... Loss: 1.452699... Val Loss: 1.370093\n",
      "Epoch: 5/5... Step: 790... Loss: 1.347421... Val Loss: 1.370110\n",
      "Epoch: 5/5... Step: 800... Loss: 1.274687... Val Loss: 1.370139\n",
      "Epoch: 5/5... Step: 810... Loss: 1.587121... Val Loss: 1.370210\n",
      "Epoch: 5/5... Step: 820... Loss: 1.426034... Val Loss: 1.370291\n",
      "Epoch: 5/5... Step: 830... Loss: 1.402733... Val Loss: 1.370403\n",
      "Epoch: 5/5... Step: 840... Loss: 1.449858... Val Loss: 1.370527\n",
      "Epoch: 5/5... Step: 850... Loss: 1.632831... Val Loss: 1.370652\n",
      "Epoch: 5/5... Step: 860... Loss: 1.392941... Val Loss: 1.370794\n",
      "Epoch: 5/5... Step: 870... Loss: 1.444425... Val Loss: 1.370926\n",
      "Epoch: 5/5... Step: 880... Loss: 1.432374... Val Loss: 1.371049\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 5 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 10\n",
    "\n",
    "train(arg_net, arg_train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 10... Loss: 1.029313... Val Loss: 1.078715\n",
      "Epoch: 1/2... Step: 20... Loss: 1.090346... Val Loss: 1.078658\n",
      "Epoch: 1/2... Step: 30... Loss: 0.899229... Val Loss: 1.078598\n",
      "Epoch: 1/2... Step: 40... Loss: 1.137216... Val Loss: 1.078538\n",
      "Epoch: 2/2... Step: 50... Loss: 1.202685... Val Loss: 1.078482\n",
      "Epoch: 2/2... Step: 60... Loss: 1.170326... Val Loss: 1.078425\n",
      "Epoch: 2/2... Step: 70... Loss: 1.025374... Val Loss: 1.078367\n",
      "Epoch: 2/2... Step: 80... Loss: 1.144756... Val Loss: 1.078319\n",
      "Epoch: 2/2... Step: 90... Loss: 1.121153... Val Loss: 1.078274\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 2 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 10\n",
    "\n",
    "train(lrec_net, lrec_train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x128 and 256x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# this is approx where I noticed the validation loss stop decreasing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlrec_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrec_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[155], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, epochs, print_every)\u001b[0m\n\u001b[1;32m     22\u001b[0m net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# get the output from the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# calculate the loss and perform backprop\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n",
      "File \u001b[0;32m~/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 33\u001b[0m, in \u001b[0;36mBiLSTMSentiment.forward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(sentence)\n\u001b[1;32m     32\u001b[0m _, (lstm_out, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m---> 33\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden2label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# log_probs = F.log_softmax(logits)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x128 and 256x3)"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 2 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 10\n",
    "\n",
    "train(lrec_lstm, lrec_train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "There are a few ways to test this network.\n",
    "\n",
    "* **Test data performance:** First, I'll see how our trained model performs on all of the defined test_data, above; I'll calculate the average loss and accuracy over the test data.\n",
    "\n",
    "* **Inference on user-generated data:** Second, I'll see if I can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.070\n",
      "Test accuracy: 0.512\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "pred_tensor = None\n",
    "label_tensor = None\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.argmax(output, dim=1)  # argmax\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    if pred_tensor == None:\n",
    "        pred_tensor = pred\n",
    "    else:\n",
    "        pred_tensor = torch.cat((pred_tensor, pred), dim=-1)\n",
    "\n",
    "    if label_tensor == None:\n",
    "        label_tensor = labels\n",
    "    else:\n",
    "        label_tensor = torch.cat((label_tensor, labels), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 0, 0,\n",
       "        0, 1, 2, 1, 2, 2, 1, 1, 1, 0, 0, 1, 2, 1, 2, 1, 1, 2, 0, 1, 0, 1, 2, 1,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro F1: 0.512\n",
      "macro F1: 0.226\n",
      "weighted F1: 0.347\n",
      "\n",
      "micro precision: 0.512\n",
      "macro precision: 0.171\n",
      "weighted precision: 0.262\n",
      "\n",
      "micro recall: 0.512\n",
      "macro recall: 0.333\n",
      "weighted recall: 0.512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ARC\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional import multiclass_precision\n",
    "from torcheval.metrics.functional import multiclass_recall\n",
    "\n",
    "print(\"micro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted F1: {:.3f}\\n\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "print(\"micro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted precision: {:.3f}\\n\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "print(\"micro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted recall: {:.3f}\\n\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro F1: 0.145\n",
      "macro F1: 0.085\n",
      "weighted F1: 0.037\n",
      "\n",
      "micro precision: 0.145\n",
      "macro precision: 0.048\n",
      "weighted precision: 0.021\n",
      "\n",
      "micro recall: 0.145\n",
      "macro recall: 0.333\n",
      "weighted recall: 0.145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "# NU\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "pred_tensor = None\n",
    "label_tensor = None\n",
    "\n",
    "\n",
    "nu_net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in nu_test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = nu_net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.argmax(output, dim=1)  # argmax\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    if pred_tensor == None:\n",
    "        pred_tensor = pred\n",
    "    else:\n",
    "        pred_tensor = torch.cat((pred_tensor, pred), dim=-1)\n",
    "\n",
    "    if label_tensor == None:\n",
    "        label_tensor = labels\n",
    "    else:\n",
    "        label_tensor = torch.cat((label_tensor, labels), dim=-1)\n",
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional import multiclass_precision\n",
    "from torcheval.metrics.functional import multiclass_recall\n",
    "\n",
    "print(\"micro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted F1: {:.3f}\\n\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "print(\"micro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted precision: {:.3f}\\n\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "print(\"micro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted recall: {:.3f}\\n\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "# # -- stats! -- ##\n",
    "# # avg test loss\n",
    "# print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# # accuracy over all test data\n",
    "# test_acc = num_correct/len(test_loader.dataset)\n",
    "# print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro F1: 0.439\n",
      "macro F1: 0.208\n",
      "weighted F1: 0.427\n",
      "\n",
      "micro precision: 0.439\n",
      "macro precision: 0.249\n",
      "weighted precision: 0.465\n",
      "\n",
      "micro recall: 0.439\n",
      "macro recall: 0.225\n",
      "weighted recall: 0.439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "# ARg\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "pred_tensor = None\n",
    "label_tensor = None\n",
    "\n",
    "\n",
    "arg_net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in arg_test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = arg_net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.argmax(output, dim=1)  # argmax\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    if pred_tensor == None:\n",
    "        pred_tensor = pred\n",
    "    else:\n",
    "        pred_tensor = torch.cat((pred_tensor, pred), dim=-1)\n",
    "\n",
    "    if label_tensor == None:\n",
    "        label_tensor = labels\n",
    "    else:\n",
    "        label_tensor = torch.cat((label_tensor, labels), dim=-1)\n",
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional import multiclass_precision\n",
    "from torcheval.metrics.functional import multiclass_recall\n",
    "\n",
    "print(\"micro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=4).item()))\n",
    "print(\"macro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=4, average='macro').item()))\n",
    "print(\"weighted F1: {:.3f}\\n\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=4, average='weighted').item()))\n",
    "\n",
    "print(\"micro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=4).item()))\n",
    "print(\"macro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=4, average='macro').item()))\n",
    "print(\"weighted precision: {:.3f}\\n\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=4, average='weighted').item()))\n",
    "\n",
    "print(\"micro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=4).item()))\n",
    "print(\"macro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=4, average='macro').item()))\n",
    "print(\"weighted recall: {:.3f}\\n\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=4, average='weighted').item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro F1: 0.423\n",
      "macro F1: 0.198\n",
      "weighted F1: 0.252\n",
      "\n",
      "micro precision: 0.423\n",
      "macro precision: 0.141\n",
      "weighted precision: 0.179\n",
      "\n",
      "micro recall: 0.423\n",
      "macro recall: 0.333\n",
      "weighted recall: 0.423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "# lrec\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "pred_tensor = None\n",
    "label_tensor = None\n",
    "\n",
    "\n",
    "lrec_net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in lrec_test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = lrec_net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.argmax(output, dim=1)  # argmax\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    if pred_tensor == None:\n",
    "        pred_tensor = pred\n",
    "    else:\n",
    "        pred_tensor = torch.cat((pred_tensor, pred), dim=-1)\n",
    "\n",
    "    if label_tensor == None:\n",
    "        label_tensor = labels\n",
    "    else:\n",
    "        label_tensor = torch.cat((label_tensor, labels), dim=-1)\n",
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional import multiclass_precision\n",
    "from torcheval.metrics.functional import multiclass_recall\n",
    "\n",
    "print(\"micro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro F1: {:.3f}\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted F1: {:.3f}\\n\".format(multiclass_f1_score(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "print(\"micro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro precision: {:.3f}\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted precision: {:.3f}\\n\".format(multiclass_precision(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n",
    "\n",
    "print(\"micro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3).item()))\n",
    "print(\"macro recall: {:.3f}\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3, average='macro').item()))\n",
    "print(\"weighted recall: {:.3f}\\n\".format(multiclass_recall(pred_tensor, label_tensor, num_classes=3, average='weighted').item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review\n",
    "\n",
    "You can change this test_review to any text that you want. Read it and think: is it pos or neg? Then see if your model predicts correctly!\n",
    "\n",
    "> The below `predict` code, takes in a trained `embed_lookup` table, a trained net, a plain text_review, and a sequence length, and prints out a custom statement for a positive or negative review!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# helper function to process and tokenize a single review\n",
    "def tokenize_review(embed_lookup, test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    tokenized_review = []\n",
    "    for word in test_words:\n",
    "        try:\n",
    "            idx = embed_lookup.vocab[word].index\n",
    "        except: \n",
    "            idx = 0\n",
    "        tokenized_review.append(idx)\n",
    "\n",
    "    return tokenized_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(embed_lookup, net, test_review, sequence_length=200):\n",
    "    \"\"\"\n",
    "    Predict whether a given test_review has negative or positive sentiment.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(embed_lookup, test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features([test_ints], seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output = net(feature_tensor)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on pos/neg reviews\n",
    "\n",
    "Below, I test my code on both positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparams\n",
    "seq_length=200 # good to use the length that was trained on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.000775\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "\n",
    "# test negative review\n",
    "predict(embed_lookup, net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.992333\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "\n",
    "predict(embed_lookup, net, test_review_pos, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out test reviews of your own!\n",
    "\n",
    "Now that you have a trained model and a predict function, you can pass in _any_ kind of text and this model will predict whether the text has a positive or negative sentiment.\n",
    "\n",
    "---\n",
    "## Further reading\n",
    "\n",
    "More than text classification, CNNs are used to analyze sequential data in a number of ways! Here are a couple of papers and applications that I find really interesting:\n",
    "* CNN for semantic representations and **search query retrieval**, [paper (Microsoft)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/www2014_cdssm_p07.pdf).\n",
    "* CNN for **genetic mutation detection**, [paper (Nature)](https://www.nature.com/articles/s41467-019-09027-x).\n",
    "* CNN for classifying [whale sounds](https://ai.googleblog.com/2018/10/acoustic-detection-of-humpback-whales.html) via spectogram and for [**audio classification**, generally (Google AI)](https://ai.google/research/pubs/pub45611)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
